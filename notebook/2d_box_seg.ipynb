{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Training a 2D bbox to 2D segmentation model\n",
    "---"
   ],
   "id": "7acbf8c49883ba50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "6c8827d5c971ef1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from evaluation.eval_utils import get_seg_bbox\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nrrd\n",
    "import SimpleITK as sitk\n",
    "from utils.plot import transparent_cmap"
   ],
   "id": "7b960d3bfd8b1693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "data_folder = Path(\"/media/liushifeng/KINGSTON/ULS Jan 2025/ULS23/novel_data/ULS23_DeepLesion3D\")\n",
    "seg_folder = data_folder / \"labels\"\n",
    "ct_folder = data_folder / \"images\"\n",
    "\n",
    "with open(data_folder / \"train.txt\", \"r\") as f:\n",
    "    train_names = [x.strip() for x in f.readlines()]\n",
    "with open(data_folder / \"val.txt\", \"r\") as f:\n",
    "    val_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "filenames = [x for x in os.listdir(seg_folder) if \".zip\" not in x]\n",
    "seg_paths = {x: seg_folder / x for x in filenames}\n",
    "ct_paths = {x: ct_folder / x for x in filenames}"
   ],
   "id": "c30e0214b5bbe9f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ignored_train_samples = [x.strip() for x in\n",
    "    \"\"\"\n",
    "    000148_04_01_034_lesion_01\n",
    "    003287_01_01_188_lesion_01\n",
    "    003931_01_01_078_lesion_01\n",
    "    000026_06_01_257_lesion_01\n",
    "    000346_01_01_085_lesion_01\n",
    "    000215_05_01_096_lesion_01\n",
    "    001354_04_02_305_lesion_01\n",
    "    001564_02_02_513_lesion_01\n",
    "    \"\"\".split(\"\\n\") if x.strip()]"
   ],
   "id": "cd4a56b7eb04c589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for plotting histograms\n",
    "bins = [int(x) for x in np.arange(-1000, 2001, 50)] + [2050] # Define HU bins\n",
    "hu_counter = Counter()  # Initialize the counter"
   ],
   "id": "fbc15bc7479b24ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "for every lesion, based on filename:\n",
    "- input\n",
    "    - crop (32X * 32Y)\n",
    "    - rectangle patch or border of the segment (32X * 32Y)\n",
    "    - full slice as context (fixed size e.g. 256 * 256)\n",
    "- output\n",
    "    - seg (32X * 32Y) **Blur for soft mask?**"
   ],
   "id": "6872350c925448ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from train_utils import crop_from_img\n",
    "\n",
    "output_folder = Path(\"/media/liushifeng/KINGSTON/ULS DL3D 2D dataset/\")\n",
    "\n",
    "plot = 0\n",
    "save = 0\n",
    "\n",
    "for filename in list(seg_paths.keys()):\n",
    "    if filename.split(\".\")[0] in ignored_train_samples:\n",
    "        print(\"skipped:\", filename.split(\".\")[0])\n",
    "        continue\n",
    "\n",
    "    lesion_name = filename.split(\".\")[0]\n",
    "\n",
    "    # load arrays\n",
    "    ct = sitk.ReadImage(ct_paths[filename])\n",
    "    ct_array = sitk.GetArrayFromImage(ct)\n",
    "    seg = sitk.ReadImage(seg_paths[filename])\n",
    "    seg_array = sitk.GetArrayFromImage(seg)\n",
    "\n",
    "    # get slices where there are segmentations\n",
    "    seg_slice_indices = np.where(seg_array.any(axis=(1,2)) > 0)[0]\n",
    "\n",
    "    for i in seg_slice_indices:\n",
    "        ct_slice = ct_array[i]\n",
    "        seg_slice = seg_array[i]\n",
    "\n",
    "        crop_bbox = get_seg_bbox(seg_slice)\n",
    "\n",
    "        if save:\n",
    "            print(f\"saving {lesion_name}_slice{i}\")\n",
    "            # train_or_val = \"train\" if lesion_name in train_names else \"val\"\n",
    "            # save_folder = output_folder / train_or_val\n",
    "            # save_folder.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(str(output_folder / \"images\" / f\"{lesion_name}_slice{i}.npy\"), ct_slice)\n",
    "            # nrrd.write(str(output_folder / \"slices\" / f\"{lesion_name}_slice{i}_seg\"), seg_slice)\n",
    "\n",
    "        if plot:\n",
    "            vmin, vmax = -200, 200\n",
    "            plt.imshow(ct_slice, vmin=vmin, vmax=vmax, cmap='gray')\n",
    "            plt.show()\n",
    "            plt.imshow(seg_slice)\n",
    "            plt.show()\n",
    "\n",
    "            # crops areas for plotting\n",
    "            margin = 2\n",
    "            ct_crop = crop_from_img(ct_slice, crop_bbox, margin, AIR_VALUE)\n",
    "            seg_crop = crop_from_img(seg_slice, crop_bbox, margin, 0)\n",
    "\n",
    "        # hu_counter.update(np.digitize(ct_crop.flatten(), bins))\n",
    "        # hu_counter.update(np.digitize(ct_crop[seg_crop == 1], bins))"
   ],
   "id": "a79388468058c798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if save:\n",
    "#     # save training data set\n",
    "#     train_or_val = \"train\" if lesion_name in train_names else \"val\"\n",
    "#     save_folder = output_folder / train_or_val\n",
    "#     save_folder.mkdir(parents=True, exist_ok=True)\n",
    "#\n",
    "#     np.save(save_folder / f\"{lesion_name}_slice{i}_crop\", np.array(ct_crop, dtype=np.int16))\n",
    "#     np.save(save_folder / f\"{lesion_name}_slice{i}_cropseg\", np.array(seg_crop, dtype=np.uint8))\n",
    "#     np.save(save_folder / f\"{lesion_name}_slice{i}_slice\", np.array(ct_slice, dtype=np.int16))\n",
    "#\n",
    "# if plot:\n",
    "#     _, axes = plt.subplots(1, 2, figsize=(4, 2))\n",
    "#\n",
    "#     # plot the crop and segment\n",
    "#     vmin, vmax = -200, 200\n",
    "#     axes[0].imshow(ct_crop, vmin=vmin, vmax=vmax, cmap='gray')\n",
    "#     axes[1].imshow(ct_crop, vmin=vmin, vmax=vmax, cmap='gray')\n",
    "#     axes[1].imshow(seg_crop, cmap=transparent_cmap(\"blue\"), alpha=0.4)\n",
    "#     for ax in axes:\n",
    "#         ax.axis(\"off\")\n",
    "#     plt.show()\n",
    "#\n",
    "#     # plot the entire slice\n",
    "#     _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "#     axes[0].imshow(ct_slice, cmap='gray')\n",
    "#     axes[1].imshow(ct_slice, cmap='gray')\n",
    "#     axes[1].imshow(seg_slice, cmap=transparent_cmap(\"blue\"), alpha=0.4)\n",
    "#     for ax in axes:\n",
    "#         ax.axis(\"off\")\n",
    "#     plt.show()"
   ],
   "id": "bea30ff75a18099a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,2.5));\n",
    "plt.bar([bins[int(x)] for x in hu_counter.keys()], hu_counter.values(), width=50);"
   ],
   "id": "abe8c51a273ef04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,2.5));\n",
    "plt.bar([bins[int(x)] for x in hu_counter.keys()], hu_counter.values(), width=50);"
   ],
   "id": "c40d9fd6524f4c7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.imshow(ct_slice, cmap='gray'); plt.axis(\"off\");\n",
    "plt.imshow(seg_slice, cmap='jet', alpha=0.5); plt.axis(\"off\");"
   ],
   "id": "ec58bff2499ee0bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "ca184f5857d77770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_dir = Path(\"/media/liushifeng/KINGSTON/ULS DL3D 2D dataset\")",
   "id": "fb4fa43161882564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import SegmentationDataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from functools import partial\n",
    "\n",
    "image_dir = data_dir / \"images\"\n",
    "label_dir = data_dir / \"masks\"\n",
    "all_images = sorted(os.listdir(image_dir))\n",
    "\n",
    "# random.shuffle(all_images)  # Shuffle before splitting\n",
    "# val_size = int(0.25 * len(all_images))\n",
    "# val_images = all_images[:val_size]\n",
    "# train_images = all_images[val_size:]\n",
    "\n",
    "val_images = [x for x in all_images if x.split(\"_slice\")[0] in val_names]\n",
    "train_images = [x for x in all_images if x.split(\"_slice\")[0] in train_names]"
   ],
   "id": "37f02184712b935d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# transform functions\n",
    "def clip_image(image, min_val, max_val, **kwargs):\n",
    "    return np.clip(image, min_val, max_val)\n",
    "\n",
    "def rescale_image(image, min_val, max_val, **kwargs):\n",
    "    return (image - min_val) / (max_val - min_val)\n",
    "\n",
    "def pass_through(mask, **kwargs):\n",
    "    return mask\n",
    "\n",
    "v = -500, 1000\n",
    "clip = partial(clip_image, min_val=v[0], max_val=v[1])\n",
    "rescale = partial(rescale_image, min_val=v[0], max_val=v[1])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Lambda(image=clip, mask=pass_through),\n",
    "    A.Lambda(image=rescale, mask=pass_through),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Lambda(image=clip, mask=pass_through),\n",
    "    A.Lambda(image=rescale, mask=pass_through),\n",
    "\n",
    "    # pixel\n",
    "    A.GaussianBlur(blur_limit=5, p=0.5),\n",
    "    A.GaussNoise(std_range=(0.01,0.05), per_channel=False, p=0.5),\n",
    "\n",
    "    # # spatial\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.OpticalDistortion(distort_limit=(-0.5, -0.1), p=0.5),\n",
    "    A.GridDistortion(p=0.5),  # adds some black edges?\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "image_names = None\n",
    "batch_size = 1\n",
    "workers = 1\n",
    "train_dataset = SegmentationDataset(image_dir, label_dir, train_images, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "val_dataset = SegmentationDataset(image_dir, label_dir, val_images, val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "\n",
    "# Visualizing data loader\n",
    "data = next(iter(val_loader))\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(data['image'].squeeze(), cmap='gray')\n",
    "plt.imshow(data['mask'].squeeze(), cmap=transparent_cmap(\"blue\"), alpha=0.5)\n",
    "plt.imshow(data['box_mask'].squeeze(), cmap='Greens', alpha=0.3)\n",
    "# plt.axis('off');\n",
    "plt.show()"
   ],
   "id": "f005c29dd4e4cdf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "7a149f50f5d1f802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(train_names) + len(val_names)",
   "id": "52a64016174ce151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model import UNet\n",
    "\n",
    "unet = UNet()\n",
    "early_stopping = EarlyStopping('val_loss', verbose=True)\n",
    "trainer = L.Trainer(\n",
    "    accumulate_grad_batches=32,\n",
    "    max_epochs=100,\n",
    "    limit_val_batches=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "trainer.fit(unet, train_loader, val_loader)"
   ],
   "id": "18945fe911f91775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data['mask'].mean()",
   "id": "d1f6ecca272623f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fp",
   "id": "b0d13cc4c00b0328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize data and model output\n",
    "data = next(iter(val_loader))\n",
    "out = unet.model.to(device)(data['input'].to(device)).detach().cpu().squeeze()\n",
    "out = out > 0.5\n",
    "\n",
    "tp = data['mask'].bool().squeeze() & out.bool().squeeze()\n",
    "fp = ~data['mask'].bool().squeeze() & out.bool().squeeze()\n",
    "fn = data['mask'].bool().squeeze() & ~out.bool().squeeze()\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(data['input'].squeeze()[0], cmap='gray')\n",
    "plt.imshow(tp, cmap=transparent_cmap(\"limegreen\"), alpha=0.5)\n",
    "plt.imshow(fp, cmap=transparent_cmap(\"red\"), alpha=0.5)\n",
    "plt.imshow(fn, cmap=transparent_cmap(\"blue\"), alpha=0.5)\n",
    "# plt.imshow(out, cmap=transparent_cmap(\"green\"), alpha=0.3);"
   ],
   "id": "534ad06ac9cabe1c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctseg-py3.12",
   "language": "python",
   "name": "ctseg-py3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
