{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom 3D Segmentation Model\n",
    "Use ULS DeepLesion 3D (700+ samples)\n",
    "* Split data into patches of 12.8cm x 12.8cm x 6.4cm, based on `Spacing_mm_px_` in DL_info.csv\n",
    "* Encode using CT-FM\n",
    "* Decode into segmentation mask of middle slice"
   ],
   "id": "81131a23a370aada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "\n",
    "data_folder = Path(\"/media/liushifeng/KINGSTON/nnUNet_raw/Dataset001_3dlesion\")\n",
    "train_images = data_folder / \"imagesTr\"\n",
    "train_labels = data_folder / \"labelsTr\""
   ],
   "id": "7c5ed0ab85945e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "uls_img = [x for x in os.listdir(train_images) if x.startswith(\"ULS\")]\n",
    "ap_img = [x for x in os.listdir(train_images) if x.startswith(\"AutoPET\")]\n",
    "\n",
    "# filenames = random.sample(ap_img, 5) + random.sample(uls_img, 5)\n",
    "filenames = uls_img + ap_img\n",
    "f = filenames[0]\n",
    "f"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / f\n",
    "seg_path = train_labels / f.replace(\"_0000.nii.gz\", \".nii.gz\")\n",
    "\n",
    "seg_img = sitk.ReadImage(seg_path)\n",
    "seg_data = sitk.GetArrayFromImage(seg_img)\n",
    "# if seg_data.mean() > 5e-4:\n",
    "#     print(seg_data.mean())\n",
    "#     ct_img = sitk.ReadImage(ct_path)\n",
    "#     ct_data = sitk.GetArrayFromImage(ct_img)\n",
    "#     plot(f, ct_data, seg_data)"
   ],
   "id": "fa54a43c731aacda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CT-FM Feature Extractor",
   "id": "741dcc4e1441bc22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from lighter_zoo import SegResEncoder\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImage, EnsureType, Orientation,\n",
    "    ScaleIntensityRange, CropForeground\n",
    ")"
   ],
   "id": "8deef76a32a217b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SegResEncoder.from_pretrained(\n",
    "    \"project-lighter/ct_fm_feature_extractor\")\n",
    "model.eval()"
   ],
   "id": "13eb3d77d34a946f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocess = Compose([\n",
    "    LoadImage(ensure_channel_first=True),  # Load image and ensure channel dimension\n",
    "    EnsureType(),                         # Ensure correct data type\n",
    "    Orientation(axcodes=\"SPL\"),           # Standardize orientation\n",
    "    # Scale intensity to [0,1] range, clipping outliers\n",
    "    ScaleIntensityRange(\n",
    "        a_min=-1024,    # Min HU value\n",
    "        a_max=2048,     # Max HU value\n",
    "        b_min=0,        # Target min\n",
    "        b_max=1,        # Target max\n",
    "        clip=True       # Clip values outside range\n",
    "    ),\n",
    "    CropForeground(allow_smaller=True)\n",
    "])"
   ],
   "id": "fb282a49605808fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "LoadImage(ensure_channel_first=True)(ct_path).shape",
   "id": "a79e5141409de265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess input\n",
    "input_tensor = preprocess(ct_path)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor.unsqueeze(0))[-1]\n",
    "\n",
    "    # Average pooling compressed the feature vector across all patches. If this is not desired, remove this line and\n",
    "    # use the output tensor directly which will give you the feature maps in a low-dimensional space.\n",
    "    avg_output = torch.nn.functional.adaptive_avg_pool3d(output, 1).squeeze()\n",
    "\n",
    "print(\"✅ Feature extraction completed\")\n",
    "print(f\"Output shape: {avg_output.shape}\")"
   ],
   "id": "24babb4110dc7726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Segmentation model",
   "id": "9b9bf519f2a36428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lighter_zoo import SegResNet\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImage, EnsureType, Orientation,\n",
    "    ScaleIntensityRange, CropForeground, Invert,\n",
    "    Activations, AsDiscrete, KeepLargestConnectedComponent,\n",
    "    SaveImage, Spacing\n",
    ")\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "device = \"cuda\""
   ],
   "id": "6377c4898c9ab7ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seg_model = SegResNet.from_pretrained(\n",
    "    \"project-lighter/whole_body_segmentation\",\n",
    ").to(device)"
   ],
   "id": "b857ba1d374c2b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / random.sample(ap_img, 1)[0]\n",
    "ct_img = sitk.ReadImage(ct_path)\n",
    "ct_data = sitk.GetArrayFromImage(ct_img)"
   ],
   "id": "219598b5149726a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inferer = SlidingWindowInferer(\n",
    "    roi_size=[96, 160, 160],  # Size of patches to process\n",
    "    sw_batch_size=1,          # Number of windows to process in parallel\n",
    "    overlap=0.25,            # Overlap between windows (reduces boundary artifacts)\n",
    "    mode=\"gaussian\",           # Gaussian weighting for overlap regions\n",
    "    sw_device=device,\n",
    "    device='cpu',\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = Compose([\n",
    "    LoadImage(ensure_channel_first=True),  # Load image and ensure channel dimension\n",
    "    Spacing(pixdim=(2.0, 2.0, 2.0)),\n",
    "    EnsureType(device=torch.device(\"cpu\")),                         # Ensure correct data type\n",
    "    Orientation(axcodes=\"SPL\"),           # Standardize orientation\n",
    "    ScaleIntensityRange(\n",
    "        a_min=-1024,    # Min HU value\n",
    "        a_max=2048,     # Max HU value\n",
    "        b_min=0,        # Target min\n",
    "        b_max=1,        # Target max\n",
    "        clip=True,\n",
    "    ),\n",
    "    CropForeground(allow_smaller=True),    # Remove background to reduce computation\n",
    "])\n",
    "\n",
    "# Postprocessing pipeline\n",
    "postprocess = Compose([\n",
    "    Activations(softmax=True),\n",
    "    AsDiscrete(argmax=True),  # threshold=0.1, dtype=torch.int16\n",
    "    # KeepLargestConnectedComponent(),\n",
    "    Invert(transform=preprocess),           # Restore original space\n",
    "    # SaveImage(output_dir=\"./ct_fm_output\")\n",
    "])"
   ],
   "id": "406f5d543b28be32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / ap_img[0]\n",
    "input_tensor = preprocess(ct_path)"
   ],
   "id": "317ab812475f826f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = inferer(input_tensor.unsqueeze(dim=0), seg_model.to(device))[0]\n",
    "\n",
    "# Copy metadata from input\n",
    "output.applied_operations = input_tensor.applied_operations\n",
    "output.affine = input_tensor.affine\n",
    "\n",
    "# Postprocess and save result\n",
    "result = postprocess(output[0])\n",
    "print(\"✅ Segmentation completed and saved\")"
   ],
   "id": "12a9582a43246dc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Visualize",
   "id": "c04d6327fd3cfd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_img = LoadImage()(ct_path)\n",
    "ct_img.shape"
   ],
   "id": "2ab785d28ceb4f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = result.squeeze()\n",
    "for i in range(0, res.shape[-1], 50):\n",
    "    slice = res[..., i].rot90()\n",
    "\n",
    "    if (slice > 0).sum() > 0:\n",
    "        ct_slice = ct_img[:, :, i].rot90()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "        axes[0].imshow(ct_slice, cmap=\"gray\")\n",
    "        axes[1].imshow(slice, vmin=0, vmax=117, cmap=\"gist_stern\")\n",
    "        plt.show()\n"
   ],
   "id": "94ca2a61a3239ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-tune seg model",
   "id": "24f3a264d679f76c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.nn import Conv3d\n",
    "import numpy as np\n",
    "import cc3d\n",
    "from utils.plot import transparent_cmap"
   ],
   "id": "61ba2d56334c740c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# replace head to single channel conv\n",
    "seg_model.up_layers[3].head = Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))"
   ],
   "id": "237f27f4b6545d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data processing\n",
    "* Load CT, other channels, and seg\n",
    "* Get all connected components in seg\n",
    "* For every cc:\n",
    "    * randomly sample max(1, 1% of all points) points in cc\n",
    "    * get (64,128,128) mm patch around it + pad?\n",
    "* crop the same patch in other channels (ctfm-seg, boxes, seg target)\n",
    "* resize all to (C,24,128,128) pixels (c, z, x, y)"
   ],
   "id": "ea986eb8b2afc032"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# constants\n",
    "PATCH_SIZE_MM = (64, 128, 128)  # zyx\n",
    "PATCH_DIMS = (24, 128, 128)  # zyx"
   ],
   "id": "abb3fff5f5d51b4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_centroid(coords):\n",
    "    return np.mean(coords, axis=0).round()\n",
    "\n",
    "\n",
    "def sample_points(coords):\n",
    "    centroid = get_centroid(coords)\n",
    "    coords = np.vstack([c for c in coords if np.any(c != centroid)])\n",
    "    n_points = 2\n",
    "    point_indices = np.random.choice(len(coords), n_points, replace=False)\n",
    "    sampled_points = np.vstack([coords[point_indices], centroid]).astype(int)\n",
    "    return np.unique(sampled_points, axis=0)\n",
    "\n",
    "\n",
    "def get_xyz_range(point, spacing, patch_size_mm):\n",
    "    x_size = round(patch_size_mm[2] / spacing[0])\n",
    "    y_size = round(patch_size_mm[1] / spacing[1])\n",
    "    z_size = round(patch_size_mm[0] / spacing[2])\n",
    "\n",
    "    x_start = point[2] - x_size // 2\n",
    "    x_end = x_start + x_size\n",
    "    y_start = point[1] - y_size // 2\n",
    "    y_end = y_start + y_size\n",
    "    z_start = point[0] - z_size // 2\n",
    "    z_end = z_start + z_size\n",
    "    return (x_start, x_end), (y_start, y_end), (z_start, z_end)\n",
    "\n",
    "\n",
    "def calculate_padding(array, x_range, y_range, z_range):\n",
    "    # array is zyx\n",
    "    pad_x = (max(-x_range[0], 0), max(x_range[1] - array.shape[2], 0))\n",
    "    pad_y = (max(-y_range[0], 0), max(y_range[1] - array.shape[1], 0))\n",
    "    pad_z = (max(-z_range[0], 0), max(z_range[1] - array.shape[0], 0))\n",
    "    return pad_x, pad_y, pad_z\n",
    "\n",
    "\n",
    "def resize_volume(volume, new_shape):\n",
    "    tensor = torch.from_numpy(volume).unsqueeze(0).unsqueeze(0).float()\n",
    "    resized_tensor = F.interpolate(tensor, size=new_shape, mode='trilinear')\n",
    "    return resized_tensor.squeeze().numpy()\n",
    "\n",
    "\n",
    "def get_patch(array, point, spacing, patch_dims=PATCH_DIMS, patch_size_mm=PATCH_SIZE_MM):\n",
    "    # get ranges to crop\n",
    "    x_range, y_range, z_range = get_xyz_range(point, spacing, patch_size_mm)\n",
    "\n",
    "    # pad array so it fits within the range\n",
    "    pad_x, pad_y, pad_z = calculate_padding(array, x_range, y_range, z_range)\n",
    "    array_padded = np.pad(array, (pad_z, pad_y, pad_x), mode='reflect')  # mode='constant', constant_values=pad_value\n",
    "\n",
    "    # adjust range after padding\n",
    "    z_range = [z + pad_z[0] for z in z_range]\n",
    "    y_range = [y + pad_y[0] for y in y_range]\n",
    "    x_range = [x + pad_x[0] for x in x_range]\n",
    "\n",
    "    # crop and resize\n",
    "    patch = array_padded[z_range[0]:z_range[1], y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    return resize_volume(patch, patch_dims)"
   ],
   "id": "7d44d2f417af4dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# select sample\n",
    "import random\n",
    "# f = random.sample(uls_img, 1)[0]\n",
    "# f = \"AutoPET-Lymphoma-B_PETCT_0fa313309d_CT_0000.nii.gz\"\n",
    "f = \"ULSDL3D_000441_02_01_187_lesion_01_0000.nii.gz\"\n",
    "\n",
    "# load CT and seg\n",
    "ct_path = train_images / f\n",
    "ct_img = sitk.ReadImage(ct_path)\n",
    "ct_data = sitk.GetArrayFromImage(ct_img)\n",
    "\n",
    "seg_path = train_labels / f.replace(\"_0000.nii.gz\", \".nii.gz\")\n",
    "seg_img = sitk.ReadImage(seg_path)\n",
    "seg_data = sitk.GetArrayFromImage(seg_img)\n",
    "spacing = seg_img.GetSpacing()\n",
    "\n",
    "# get connected components in seg\n",
    "labels, n_components = cc3d.connected_components(seg_data, return_N=True)\n",
    "\n",
    "# sample points within cc\n",
    "for c in range(1, n_components + 1):\n",
    "    coords = np.argwhere(labels == c)\n",
    "    points = sample_points(coords)\n",
    "    for point in points:\n",
    "        # crop volume around the point\n",
    "        seg_patch = get_patch(seg_data, point, spacing)\n",
    "        ct_patch = get_patch(ct_data, point, spacing)\n",
    "        print(f\"{f[:10]} {c=} {point=}\")"
   ],
   "id": "cd22546babaaa5a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "93e225c903543973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize slices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6, 2))\n",
    "for i in range(3):\n",
    "    axes[i].imshow(ct_patch.mean(axis=i))\n",
    "    axes[i].imshow(seg_patch.mean(axis=i), cmap=transparent_cmap(\"r\"))\n",
    "\n",
    "for ct, seg in zip(ct_patch, seg_patch):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes[0].imshow(ct, cmap=\"gray\")\n",
    "    axes[1].imshow(ct, cmap=\"gray\")\n",
    "    axes[1].imshow(seg, cmap=transparent_cmap(\"r\"))\n",
    "    plt.show()"
   ],
   "id": "4ec06bbf9d0736da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
