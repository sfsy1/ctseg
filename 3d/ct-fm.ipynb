{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Custom 3D Segmentation Model\n",
    "Use ULS DeepLesion 3D (700+ samples)\n",
    "* Split data into patches of 12.8cm x 12.8cm x 6.4cm, based on `Spacing_mm_px_` in DL_info.csv\n",
    "* Encode using CT-FM\n",
    "* Decode into segmentation mask of middle slice"
   ],
   "id": "81131a23a370aada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import SimpleITK as sitk\n",
    "\n",
    "data_folder = Path(\"/media/liushifeng/KINGSTON/nnUNet_raw/Dataset001_3dlesion\")\n",
    "train_images = data_folder / \"imagesTr\"\n",
    "train_labels = data_folder / \"labelsTr\""
   ],
   "id": "7c5ed0ab85945e49",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "uls_img = [x for x in os.listdir(train_images) if x.startswith(\"ULS\")]\n",
    "ap_img = [x for x in os.listdir(train_images) if x.startswith(\"AutoPET\")]\n",
    "\n",
    "# filenames = random.sample(ap_img, 5) + random.sample(uls_img, 5)\n",
    "filenames = uls_img + ap_img\n",
    "f = filenames[0]\n",
    "f"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / f\n",
    "seg_path = train_labels / f.replace(\"_0000.nii.gz\", \".nii.gz\")\n",
    "\n",
    "seg_img = sitk.ReadImage(seg_path)\n",
    "seg_data = sitk.GetArrayFromImage(seg_img)\n",
    "# if seg_data.mean() > 5e-4:\n",
    "#     print(seg_data.mean())\n",
    "#     ct_img = sitk.ReadImage(ct_path)\n",
    "#     ct_data = sitk.GetArrayFromImage(ct_img)\n",
    "#     plot(f, ct_data, seg_data)"
   ],
   "id": "fa54a43c731aacda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CT-FM Feature Extractor",
   "id": "741dcc4e1441bc22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from lighter_zoo import SegResEncoder\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImage, EnsureType, Orientation,\n",
    "    ScaleIntensityRange, CropForeground\n",
    ")"
   ],
   "id": "8deef76a32a217b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SegResEncoder.from_pretrained(\n",
    "    \"project-lighter/ct_fm_feature_extractor\")\n",
    "model.eval()"
   ],
   "id": "13eb3d77d34a946f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocess = Compose([\n",
    "    LoadImage(ensure_channel_first=True),  # Load image and ensure channel dimension\n",
    "    EnsureType(),                         # Ensure correct data type\n",
    "    Orientation(axcodes=\"SPL\"),           # Standardize orientation\n",
    "    # Scale intensity to [0,1] range, clipping outliers\n",
    "    ScaleIntensityRange(\n",
    "        a_min=-1024,    # Min HU value\n",
    "        a_max=2048,     # Max HU value\n",
    "        b_min=0,        # Target min\n",
    "        b_max=1,        # Target max\n",
    "        clip=True       # Clip values outside range\n",
    "    ),\n",
    "    CropForeground(allow_smaller=True)\n",
    "])"
   ],
   "id": "fb282a49605808fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "LoadImage(ensure_channel_first=True)(ct_path).shape",
   "id": "a79e5141409de265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess input\n",
    "input_tensor = preprocess(ct_path)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor.unsqueeze(0))[-1]\n",
    "\n",
    "    # Average pooling compressed the feature vector across all patches. If this is not desired, remove this line and\n",
    "    # use the output tensor directly which will give you the feature maps in a low-dimensional space.\n",
    "    avg_output = torch.nn.functional.adaptive_avg_pool3d(output, 1).squeeze()\n",
    "\n",
    "print(\"✅ Feature extraction completed\")\n",
    "print(f\"Output shape: {avg_output.shape}\")"
   ],
   "id": "24babb4110dc7726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Segmentation model",
   "id": "9b9bf519f2a36428"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lighter_zoo import SegResNet\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImage, EnsureType, Orientation,\n",
    "    ScaleIntensityRange, CropForeground, Invert,\n",
    "    Activations, AsDiscrete, KeepLargestConnectedComponent,\n",
    "    SaveImage, Spacing\n",
    ")\n",
    "from monai.inferers import SlidingWindowInferer\n",
    "device = \"cuda\""
   ],
   "id": "6377c4898c9ab7ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seg_model = SegResNet.from_pretrained(\n",
    "    \"project-lighter/whole_body_segmentation\",\n",
    ").to(device)"
   ],
   "id": "b857ba1d374c2b1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import random",
   "id": "34cc7a29cf404dc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / random.sample(ap_img, 1)[0]\n",
    "ct_img = sitk.ReadImage(ct_path)\n",
    "ct_data = sitk.GetArrayFromImage(ct_img)"
   ],
   "id": "219598b5149726a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ct_data.shape",
   "id": "9763ca1d4522b6af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import joblib\n",
   "id": "bb7551633901700b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joblib.load(r\"/media/liushifeng/KINGSTON/nnUNet_preprocessed/Dataset001_3dlesion/nnUNetPlans_3d_fullres/AutoPET-Lymphoma-B_PETCT_0f4ee9e078_CT.pkl\")",
   "id": "a930602cbb3bb0dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(ct_data.mean(axis=1))",
   "id": "d749b01eb81f1372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "676 * 2.5",
   "id": "91105ba794b1def2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "54c6526b0b393c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ct_img.GetSize(), ct_img.GetSpacing()",
   "id": "df4219dff512004e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inferer = SlidingWindowInferer(\n",
    "    roi_size=[96, 160, 160],  # Size of patches to process\n",
    "    sw_batch_size=1,          # Number of windows to process in parallel\n",
    "    overlap=0.25,            # Overlap between windows (reduces boundary artifacts)\n",
    "    mode=\"gaussian\",           # Gaussian weighting for overlap regions\n",
    "    sw_device=device,\n",
    "    device='cpu',\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocess = Compose([\n",
    "    LoadImage(ensure_channel_first=True),  # Load image and ensure channel dimension\n",
    "    Spacing(pixdim=(2.0, 2.0, 2.0)),\n",
    "    EnsureType(device=torch.device(\"cpu\")),                         # Ensure correct data type\n",
    "    Orientation(axcodes=\"SPL\"),           # Standardize orientation\n",
    "    ScaleIntensityRange(\n",
    "        a_min=-1024,    # Min HU value\n",
    "        a_max=2048,     # Max HU value\n",
    "        b_min=0,        # Target min\n",
    "        b_max=1,        # Target max\n",
    "        clip=True,\n",
    "    ),\n",
    "    CropForeground(allow_smaller=True),    # Remove background to reduce computation\n",
    "])\n",
    "\n",
    "# Postprocessing pipeline\n",
    "postprocess = Compose([\n",
    "    Activations(softmax=True),\n",
    "    AsDiscrete(argmax=True),  # threshold=0.1, dtype=torch.int16\n",
    "    # KeepLargestConnectedComponent(),\n",
    "    Invert(transform=preprocess),           # Restore original space\n",
    "    # SaveImage(output_dir=\"./ct_fm_output\")\n",
    "])"
   ],
   "id": "406f5d543b28be32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_path = train_images / ap_img[0]\n",
    "input_tensor = preprocess(ct_path)"
   ],
   "id": "317ab812475f826f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_tensor.shape",
   "id": "faf33212781a634",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "input_tensor.shape",
   "id": "fa4bc4a05e7f194d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = inferer(input_tensor.unsqueeze(dim=0), seg_model.to(device))[0]\n",
    "\n",
    "# Copy metadata from input\n",
    "output.applied_operations = input_tensor.applied_operations\n",
    "output.affine = input_tensor.affine\n",
    "\n",
    "# Postprocess and save result\n",
    "result = postprocess(output[0])\n",
    "print(\"✅ Segmentation completed and saved\")"
   ],
   "id": "12a9582a43246dc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Visualize",
   "id": "c04d6327fd3cfd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ct_img = LoadImage()(ct_path)\n",
    "ct_img.shape"
   ],
   "id": "2ab785d28ceb4f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = result.squeeze()\n",
    "for i in range(0, res.shape[-1], 50):\n",
    "    slice = res[..., i].rot90()\n",
    "\n",
    "    if (slice > 0).sum() > 0:\n",
    "        ct_slice = ct_img[:, :, i].rot90()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "        axes[0].imshow(ct_slice, cmap=\"gray\")\n",
    "        axes[1].imshow(slice, vmin=0, vmax=117, cmap=\"gist_stern\")\n",
    "        plt.show()\n"
   ],
   "id": "94ca2a61a3239ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-tune seg model",
   "id": "24f3a264d679f76c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torch.nn import Conv3d",
   "id": "61ba2d56334c740c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# replace head to single channel conv\n",
    "seg_model.up_layers[3].head = Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))"
   ],
   "id": "237f27f4b6545d0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seg_path = train_labels / random.sample(ap_img, 1)[0].replace(\"_0000.nii.gz\", \".nii.gz\")\n",
    "seg_img = sitk.ReadImage(seg_path)\n",
    "seg_data = sitk.GetArrayFromImage(seg_img)"
   ],
   "id": "ae80da88438508b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.imshow(seg_data.any(axis=1))",
   "id": "e4db1e0772a85fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cc3d\n",
    "\n",
    "\n",
    "# Label connected components (26-connected by default)\n",
    "labels, num_components = cc3d.connected_components(seg_data, return_N=True)\n",
    "\n",
    "# Calculate centroids\n",
    "centroids = []\n",
    "for component_id in range(1, num_components + 1):  # Skip background (label 0)\n",
    "    voxel_coords = np.argwhere(labels == component_id)\n",
    "    centroid = np.mean(voxel_coords, axis=0)  # [z, y, x] order\n",
    "    centroids.append(centroid)"
   ],
   "id": "c697a7d8b64bb982",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "centroids",
   "id": "e57d54e81e0eccd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
